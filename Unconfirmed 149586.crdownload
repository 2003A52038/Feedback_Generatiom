#!/usr/bin/env python
# coding: utf-8

# In[4]:


import pandas as pd
# Load your dataset from Google Drive
df = pd.read_csv("OS-CUM-dataset.csv")
df.head()


# In[5]:


df.shape


# In[6]:


import pandas as pd

# Assuming you have your dataset loaded into a pandas DataFrame called 'df'

# Sample x% of responses from each question
x_percent = 0.10  # You can adjust this value as needed
sampled_data = []

for question_id in range(268):
    # Select x% of responses for each question
    question_data = df[df['ID'] == question_id].sample(frac=x_percent, random_state=42)
    sampled_data.append(question_data)

# Concatenate sampled data into a single DataFrame
dd = pd.concat(sampled_data)


# In[7]:


dd


# In[8]:


dd.columns


# In[9]:


dd=dd.drop(['ID','MARKS','Unnamed: 5'],axis=1)


# In[10]:


dd.columns


# In[11]:


ddd=dd.dropna()


# In[12]:


print(ddd.isnull().sum())


# In[13]:


ddd.shape


# In[14]:


data = ddd.reset_index(drop=True)
data


# In[15]:


pip install transformers


# In[16]:


pip install evaluate


# In[17]:


pip install rouge


# In[18]:


pip install torch


# In[21]:


pip install torch torchvision torchaudio


# In[22]:


pip install spacy


# In[23]:


get_ipython().system('python -m spacy download en_core_web_sm')


# In[24]:


import torch
import json
from tqdm import tqdm
import torch.nn as nn
from torch.optim import Adam
import nltk
import spacy
import string
import evaluate  # Bleu
from torch.utils.data import Dataset, DataLoader, RandomSampler
import pandas as pd
import numpy as np
import transformers
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from transformers import T5Tokenizer, T5Model, T5ForConditionalGeneration, T5TokenizerFast
from transformers import get_linear_schedule_with_warmup

import warnings
warnings.filterwarnings("ignore")


# In[25]:


dataset = pd.DataFrame(data)
dataset


# In[27]:


TOKENIZER = T5TokenizerFast.from_pretrained("t5-small")
MODEL = T5ForConditionalGeneration.from_pretrained("t5-small", return_dict=True)
OPTIMIZER = Adam(MODEL.parameters(), lr=0.00001)
R_LEN = 256   # Responce Length
F_LEN = 32    # Feedback Length
BATCH_SIZE = 4
DEVICE = "cuda:0"


# In[28]:


class QA_Dataset(Dataset):
    def __init__(self, tokenizer, dataframe, r_len, f_len):
        self.tokenizer = tokenizer
        self.r_len = r_len
        self.f_len = f_len
        self.dataset = dataframe
        self.prompt = self.dataset["PROMPT"]
        self.response = self.dataset["RESPONSE"]
        self.feedback = self.dataset['FEEDBACK']

    def __len__(self):
        return len(self.prompt)
    def __getitem__(self, idx):
        prompt = self.prompt.iloc[idx]
        response = self.response.iloc[idx]
        feedback = self.feedback.iloc[idx]


        prompt_tokenized = self.tokenizer(prompt, response, max_length=self.r_len, padding="max_length",
                                                    truncation=True, pad_to_max_length=True, add_special_tokens=True)
        feedback_tokenized = self.tokenizer(feedback, max_length=self.f_len, padding="max_length",
                                          truncation=True, pad_to_max_length=True, add_special_tokens=True)

        labels = torch.tensor(feedback_tokenized["input_ids"], dtype=torch.long)
        labels[labels == 0] = -100

        return {
            "input_ids": torch.tensor(prompt_tokenized["input_ids"], dtype=torch.long),
            "attention_mask": torch.tensor(prompt_tokenized["attention_mask"], dtype=torch.long),
            "labels": labels,
            "decoder_attention_mask": torch.tensor(feedback_tokenized["attention_mask"], dtype=torch.long)
        }


# In[29]:


# Dataloader
train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)

# Create datasets and data loaders
train_dataset = QA_Dataset(TOKENIZER, train_data, R_LEN, F_LEN)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)


# In[30]:


DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")  # Adjust device accordingly


# In[31]:


import matplotlib.pyplot as plt
from tqdm import tqdm

NUM_EPOCHS = 5 # Change this value to adjust the number of epochs
train_losses = []

for epoch in range(NUM_EPOCHS):
    MODEL.train()
    train_loss = 0  # Initialize train_loss for each epoch
    train_batch_count = 0  # Initialize train_batch_count for each epoch
    for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS}, Training batches", mininterval=2):
        input_ids = batch["input_ids"].to(DEVICE)
        attention_mask = batch["attention_mask"].to(DEVICE)
        labels = batch["labels"].to(DEVICE)
        decoder_attention_mask = batch["decoder_attention_mask"].to(DEVICE)

        # Ensure all tensors are on the same device
        MODEL.to(DEVICE)

        outputs = MODEL(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels,
            decoder_attention_mask=decoder_attention_mask
        )

        OPTIMIZER.zero_grad()
        outputs.loss.backward()
        OPTIMIZER.step()
        train_loss += outputs.loss.item()
        train_batch_count += 1

    average_train_loss = train_loss / train_batch_count
    train_losses.append(average_train_loss)
    print(f"Epoch {epoch+1}/{NUM_EPOCHS} -> Average Train Loss: {average_train_loss}")


# In[32]:


plt.figure(figsize=(10, 6))  # Adjust width and height as needed
plt.plot(range(1, NUM_EPOCHS+1), train_losses, marker='o', linestyle='-')
plt.xlabel('Epoch')
plt.ylabel('Training Loss')
plt.title('Training Loss vs Epochs')
plt.grid(True)
plt.show()


# In[38]:


import os

# Define the directory path where you want to save the trained model
model_save_path = "saved_models/"
os.makedirs(model_save_path, exist_ok=True)

# Save the model and tokenizer
MODEL.save_pretrained(model_save_path)
TOKENIZER.save_pretrained(model_save_path)


# In[39]:


from transformers import T5ForConditionalGeneration, T5TokenizerFast

# Define the directory path where you saved the trained model
model_save_path = "saved_models/"

# Load the model and tokenizer
loaded_model = T5ForConditionalGeneration.from_pretrained(model_save_path)
loaded_tokenizer = T5TokenizerFast.from_pretrained(model_save_path)


# In[40]:


import torch
from transformers import T5TokenizerFast

# Assuming Q_LEN is defined somewhere in your code
Q_LEN = 512  # Adjust this value according to your requirements

def predict_feedback( PROMPT,RESPONSE, ref_feedback=None):
    inputs = loaded_tokenizer(PROMPT, RESPONSE, max_length=Q_LEN, padding="max_length", truncation=True, return_tensors="pt")

    input_ids = inputs["input_ids"].to(DEVICE)
    attention_mask = inputs["attention_mask"].to(DEVICE)

    outputs = loaded_model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=Q_LEN, num_beams=4, early_stopping=True)

    predicted_feedback = loaded_tokenizer.decode(outputs[0], skip_special_tokens=True)

    if ref_feedback:
        # Load the Bleu metric (assuming you've already defined and loaded it somewhere in your code)
        bleu = evaluate.load("google_bleu")
        score = bleu.compute(predictions=[predicted_feedback], references=[ref_feedback])
    
        print("PROMPT: \n", PROMPT)
        print("\n")
        print("RESPONSE: \n", RESPONSE)
        
        return {
            "Reference feedback: ": ref_feedback, 
            "Predicted feedback: ": predicted_feedback, 
            "BLEU Score: ": score
        }
    else:
        return predicted_feedback


# In[41]:


PROMPT = dataset.iloc[0]["PROMPT"]
RESPONSE = dataset.iloc[0]["RESPONSE"]
FEEDBACK = dataset.iloc[0]["FEEDBACK"]
predict_feedback(PROMPT,RESPONSE,FEEDBACK)


# In[42]:


import torch
from transformers import T5TokenizerFast
from nltk.translate import bleu_score

# Assuming Q_LEN is defined somewhere in your code
Q_LEN = 512  # Adjust this value according to your requirements

def calculate_bleu_score(PROMPT,RESPONSE, ref_feedback):
    inputs = loaded_tokenizer(PROMPT,RESPONSE, max_length=Q_LEN, padding="max_length", truncation=True, return_tensors="pt")

    input_ids = inputs["input_ids"].to(DEVICE)
    attention_mask = inputs["attention_mask"].to(DEVICE)

    outputs = loaded_model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=Q_LEN, num_beams=4, early_stopping=True)

    predicted_feedback = loaded_tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Compute BLEU score
    return bleu_score.sentence_bleu([ref_feedback.split()], predicted_feedback.split())

# Assuming dataset is your test dataset
bleu_scores = []
for i in range(int(len(dataset) * 0.2)):  # Considering 20% of the test dataset
    PROMPT = dataset.iloc[i]["PROMPT"]
    RESPONSE = dataset.iloc[i]["RESPONSE"]
    FEEDBACK = dataset.iloc[i]["FEEDBACK"]
    score = calculate_bleu_score(RESPONSE, PROMPT, FEEDBACK)
    bleu_scores.append(score)

# Print or use bleu_scores as needed
print(bleu_scores)


# In[43]:


import matplotlib.pyplot as plt

# Assuming bleu_scores contains the BLEU scores calculated for each pair in the dataset

# Plotting the BLEU scores
plt.figure(figsize=(10, 6)) 
plt.plot(range(len(bleu_scores)), bleu_scores, marker='o', linestyle='-')
plt.xlabel('Pair Index')
plt.ylabel('BLEU Score')
plt.title('BLEU Scores for Test Dataset Pairs')
plt.grid(True)
plt.show()


# In[44]:


average_bleu_score = sum(bleu_scores) / len(bleu_scores)

print("Average BLEU Score:", average_bleu_score)


# In[46]:


import torch
from transformers import BertTokenizer, BertModel
from scipy.spatial.distance import cosine

# Load pre-trained BERT model and tokenizer
bert_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Define a function to compute BERT embeddings
def get_bert_embeddings(text):
    tokens = bert_tokenizer.tokenize(text)
    indexed_tokens = bert_tokenizer.convert_tokens_to_ids(tokens)
    segments_ids = [1] * len(tokens)
    tokens_tensor = torch.tensor([indexed_tokens])
    segments_tensor = torch.tensor([segments_ids])
    with torch.no_grad():
        outputs = bert_model(tokens_tensor, segments_tensor)
        hidden_states = outputs[2]
        token_embeddings = torch.stack(hidden_states, dim=0)
        token_embeddings = torch.squeeze(token_embeddings, dim=1)
        token_embeddings = token_embeddings.permute(1,0,2)
        token_vecs = token_embeddings[-4:,:,:]
        sentence_embedding = torch.mean(token_vecs, dim=0)
        return sentence_embedding

# Define a function to compute BERT similarity scores
def calculate_bert_similarity(response, prompt, ref_answer):
    # Compute BERT embeddings
    response_embedding = get_bert_embeddings(response)
    ref_embedding = get_bert_embeddings(ref_answer)
    # Flatten embeddings
    response_embedding = response_embedding.flatten().numpy()
    ref_embedding = ref_embedding.flatten().numpy()
    # Compute cosine similarity between embeddings
    similarity_score = 1 - cosine(response_embedding, ref_embedding)
    return similarity_score

# Assuming dataset is your test dataset
bert_similarity_scores = []
for i in range(int(len(dataset) * 0.2)):  # Considering 20% of the test dataset
    RESPONSE = dataset.iloc[i]["RESPONSE"]
    PROMPT = dataset.iloc[i]["PROMPT"]
    FEEDBACK = dataset.iloc[i]["FEEDBACK"]
    score = calculate_bert_similarity(RESPONSE, PROMPT, FEEDBACK)
    bert_similarity_scores.append(score)

# Print or use bert_similarity_scores as needed
print(bert_similarity_scores)


# In[47]:


import matplotlib.pyplot as plt

# Assuming bert_similarity_scores is a list containing BERT similarity scores
plt.figure(figsize=(10, 6))
plt.plot(bert_similarity_scores, marker='o', linestyle='-', color='b')
plt.xlabel('Index')
plt.ylabel('BERT Similarity Score')
plt.title('BERT Similarity Scores for Test Dataset')
plt.grid(True)
plt.show()


# In[61]:


import matplotlib.pyplot as plt

# Assuming bert_similarity_scores is a list containing BERT similarity scores
plt.figure(figsize=(10, 6))
plt.scatter(range(len(bert_similarity_scores)), bert_similarity_scores, color='b', alpha=0.5)
plt.xlabel('Index')
plt.ylabel('BERT Similarity Score')
plt.title('BERT Similarity Scores for Test Dataset')
plt.grid(True)
plt.show()


# In[48]:


average_bert_similarity_score = sum(bert_similarity_scores) / len(bert_similarity_scores)

# Print or use average_bert_similarity_score as needed
print("Average BERT similarity score:", average_bert_similarity_score)


# In[49]:


pip install rouge-score


# In[64]:


import matplotlib.pyplot as plt

# Assuming bleu_scores contains the BLEU scores calculated for each pair in the dataset
# Assuming bert_similarity_scores is a list containing BERT similarity scores

plt.figure(figsize=(10, 6))

# Plotting BLEU scores
plt.scatter(range(len(bleu_scores)), bleu_scores, color='b', label='BLEU Score')

# Plotting BERT similarity scores
plt.scatter(range(len(bert_similarity_scores)), bert_similarity_scores, color='r', label='BERT Similarity Score')

# Adding labels and title
plt.xlabel('Pair Index')
plt.ylabel('Score')
plt.title('Comparison of BLEU Scores and BERT Similarity Scores')
plt.legend()
plt.grid(True)
plt.show()


# In[50]:


from rouge_score import rouge_scorer

def calculate_rouge_scores(response, reference):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    scores = scorer.score(response, reference)
    return scores

# Assuming dataset is your test dataset
rouge1_scores = []
rouge2_scores = []
rougeL_scores = []

for i in range(int(len(dataset) * 0.2)):  # Considering 20% of the test dataset
    RESPONSE = dataset.iloc[i]["RESPONSE"]
    REF_FEEDBACK = dataset.iloc[i]["FEEDBACK"]
    scores = calculate_rouge_scores(RESPONSE, REF_FEEDBACK)
    rouge1_scores.append(scores['rouge1'].fmeasure)
    rouge2_scores.append(scores['rouge2'].fmeasure)
    rougeL_scores.append(scores['rougeL'].fmeasure)

# Print or use rouge scores as needed
print("ROUGE-1 F1 scores:", rouge1_scores)
print("ROUGE-2 F1 scores:", rouge2_scores)
print("ROUGE-L F1 scores:", rougeL_scores)


# In[51]:


# Calculate average ROUGE scores
avg_rouge1_score = sum(rouge1_scores) / len(rouge1_scores)
avg_rouge2_score = sum(rouge2_scores) / len(rouge2_scores)
avg_rougeL_score = sum(rougeL_scores) / len(rougeL_scores)

# Print or use average ROUGE scores as needed
print("Average ROUGE-1 F1 score:", avg_rouge1_score)
print("Average ROUGE-2 F1 score:", avg_rouge2_score)
print("Average ROUGE-L F1 score:", avg_rougeL_score)


# In[52]:


# Assuming dataset is your test dataset
rouge1_recall = []
rouge1_precision = []
rouge1_f1 = []

rouge2_recall = []
rouge2_precision = []
rouge2_f1 = []

rougeL_recall = []
rougeL_precision = []
rougeL_f1 = []

for i in range(int(len(dataset) * 0.2)):  # Considering 20% of the test dataset
    RESPONSE = dataset.iloc[i]["RESPONSE"]
    REF_FEEDBACK = dataset.iloc[i]["FEEDBACK"]
    scores = calculate_rouge_scores(RESPONSE, REF_FEEDBACK)
    
    # ROUGE-1
    rouge1_recall.append(scores['rouge1'].recall)
    rouge1_precision.append(scores['rouge1'].precision)
    rouge1_f1.append(scores['rouge1'].fmeasure)
    
    # ROUGE-2
    rouge2_recall.append(scores['rouge2'].recall)
    rouge2_precision.append(scores['rouge2'].precision)
    rouge2_f1.append(scores['rouge2'].fmeasure)
    
    # ROUGE-L
    rougeL_recall.append(scores['rougeL'].recall)
    rougeL_precision.append(scores['rougeL'].precision)
    rougeL_f1.append(scores['rougeL'].fmeasure)

import numpy as np

# Calculate average scores for ROUGE-1
avg_rouge1_recall = np.mean(rouge1_recall)
avg_rouge1_precision = np.mean(rouge1_precision)
avg_rouge1_f1 = np.mean(rouge1_f1)

# Calculate average scores for ROUGE-2
avg_rouge2_recall = np.mean(rouge2_recall)
avg_rouge2_precision = np.mean(rouge2_precision)
avg_rouge2_f1 = np.mean(rouge2_f1)

# Calculate average scores for ROUGE-L
avg_rougeL_recall = np.mean(rougeL_recall)
avg_rougeL_precision = np.mean(rougeL_precision)
avg_rougeL_f1 = np.mean(rougeL_f1)

# Print or use the average scores
print("Average ROUGE-1 Recall:", avg_rouge1_recall)
print("Average ROUGE-1 Precision:", avg_rouge1_precision)
print("Average ROUGE-1 F1-score:", avg_rouge1_f1)

print("Average ROUGE-2 Recall:", avg_rouge2_recall)
print("Average ROUGE-2 Precision:", avg_rouge2_precision)
print("Average ROUGE-2 F1-score:", avg_rouge2_f1)

print("Average ROUGE-L Recall:", avg_rougeL_recall)
print("Average ROUGE-L Precision:", avg_rougeL_precision)
print("Average ROUGE-L F1-score:", avg_rougeL_f1)


# In[57]:


import matplotlib.pyplot as plt

# Plotting ROUGE scores
plt.figure(figsize=(10, 6))

# Plot individual ROUGE scores
plt.subplot(3, 1, 1)
plt.plot(range(len(rouge1_scores)), rouge1_scores, marker='o', linestyle='-')
plt.xlabel('Index')
plt.ylabel('ROUGE-1 F1 Score')
plt.title('ROUGE-1 F1 Score for test dataset')

plt.subplot(3, 1, 2)
plt.plot(range(len(rouge2_scores)), rouge2_scores, marker='o', linestyle='-')
plt.xlabel('Index')
plt.ylabel('ROUGE-2 F1 Score')
plt.title('ROUGE-2 F1 Score for test dataset')

plt.subplot(3, 1, 3)
plt.plot(range(len(rougeL_scores)), rougeL_scores, marker='o', linestyle='-')
plt.xlabel('Index')
plt.ylabel('ROUGE-L F1 Score')
plt.title('ROUGE-L F1 Score for test dataset')

# Plot average ROUGE scores
plt.axhline(y=avg_rouge1_score, color='r', linestyle='--', label=f'Avg ROUGE-1 F1: {avg_rouge1_score:.4f}')
plt.axhline(y=avg_rouge2_score, color='g', linestyle='--', label=f'Avg ROUGE-2 F1: {avg_rouge2_score:.4f}')
plt.axhline(y=avg_rougeL_score, color='b', linestyle='--', label=f'Avg ROUGE-L F1: {avg_rougeL_score:.4f}')

plt.legend()

plt.tight_layout()
plt.show()


# In[59]:


import matplotlib.pyplot as plt

# Plotting ROUGE precision scores
plt.figure(figsize=(10, 6))

# Plot individual ROUGE precision scores
plt.subplot(3, 1, 1)
plt.plot(range(len(rouge1_precision)), rouge1_precision, marker='o', linestyle='-')
plt.xlabel('Index')
plt.ylabel('ROUGE-1 Precision')
plt.title('ROUGE-1 Precision for test dataset')

plt.subplot(3, 1, 2)
plt.plot(range(len(rouge2_precision)), rouge2_precision, marker='o', linestyle='-')
plt.xlabel('Index')
plt.ylabel('ROUGE-2 Precision')
plt.title('ROUGE-2 Precision for test dataset')

plt.subplot(3, 1, 3)
plt.plot(range(len(rougeL_precision)), rougeL_precision, marker='o', linestyle='-')
plt.xlabel('Index')
plt.ylabel('ROUGE-L Precision')
plt.title('ROUGE-L Precision for test dataset')

# Plot average ROUGE precision scores
plt.axhline(y=avg_rouge1_precision, color='r', linestyle='--', label=f'Avg ROUGE-1 Precision: {avg_rouge1_precision:.4f}')
plt.axhline(y=avg_rouge2_precision, color='g', linestyle='--', label=f'Avg ROUGE-2 Precision: {avg_rouge2_precision:.4f}')
plt.axhline(y=avg_rougeL_precision, color='b', linestyle='--', label=f'Avg ROUGE-L Precision: {avg_rougeL_precision:.4f}')

plt.legend()

plt.tight_layout()
plt.show()


# In[60]:


import matplotlib.pyplot as plt

# Plotting ROUGE recall scores
plt.figure(figsize=(10, 6))

# Plot individual ROUGE recall scores
plt.subplot(3, 1, 1)
plt.plot(range(len(rouge1_recall)), rouge1_recall, marker='o', linestyle='-')
plt.xlabel('Index')
plt.ylabel('ROUGE-1 Recall')
plt.title('ROUGE-1 Recall for test dataset')

plt.subplot(3, 1, 2)
plt.plot(range(len(rouge2_recall)), rouge2_recall, marker='o', linestyle='-')
plt.xlabel('Index')
plt.ylabel('ROUGE-2 Recall')
plt.title('ROUGE-2 Recall for test dataset')

plt.subplot(3, 1, 3)
plt.plot(range(len(rougeL_recall)), rougeL_recall, marker='o', linestyle='-')
plt.xlabel('Index')
plt.ylabel('ROUGE-L Recall')
plt.title('ROUGE-L Recall for test dataset')

# Plot average ROUGE recall scores
plt.axhline(y=avg_rouge1_recall, color='r', linestyle='--', label=f'Avg ROUGE-1 Recall: {avg_rouge1_recall:.4f}')
plt.axhline(y=avg_rouge2_recall, color='g', linestyle='--', label=f'Avg ROUGE-2 Recall: {avg_rouge2_recall:.4f}')
plt.axhline(y=avg_rougeL_recall, color='b', linestyle='--', label=f'Avg ROUGE-L Recall: {avg_rougeL_recall:.4f}')

plt.legend()

plt.tight_layout()
plt.show()


# In[80]:


def predict_feedback(PROMPT, RESPONSE):
    inputs = loaded_tokenizer(PROMPT, RESPONSE, max_length=Q_LEN, padding="max_length", truncation=True, return_tensors="pt")

    input_ids = inputs["input_ids"].to(DEVICE)
    attention_mask = inputs["attention_mask"].to(DEVICE)

    outputs = loaded_model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=Q_LEN, num_beams=4, early_stopping=True)

    predicted_feedback = loaded_tokenizer.decode(outputs[0], skip_special_tokens=True)

    return predicted_feedback


# In[ ]:





# In[81]:


# Example prompt and response
PROMPT = "Explain about cpu scheduling algorithm?"
RESPONSE = "CPU scheduling algorithms are methods used by operating systems to manage the execution of multiple processes on a single CPU. These algorithms determine which process should be executed next and for how long, aiming to maximize system performance and responsiveness."

# Generate feedback using the predict_feedback function
predicted_feedback = predict_feedback(PROMPT, RESPONSE)

# Print the generated feedback
print("Generated Feedback:")
print(predicted_feedback)


# In[ ]:





# In[ ]:





# In[ ]:




